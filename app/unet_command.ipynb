{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準モジュール(install不要)\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# myself\n",
    "from config import setting\n",
    "from module import const\n",
    "from module import image_loader  # get_train_transform, LoadDataSet\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "const.CHECKPOINT_PATH_UNet = setting.const.CHECKPOINT_PATH + \"/UNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1686121211369,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "np6tIMs5_Lz8"
   },
   "outputs": [],
   "source": [
    "const.IMG_HEIGHT = 256\n",
    "const.IMG_WIDTH = 256\n",
    "\n",
    "# const.NUM_EPOCHS = 300\n",
    "const.NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1686121118929,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "1nLMch089N2C",
    "outputId": "52518083-4912-4dae-9349-38b4b251af50"
   },
   "outputs": [],
   "source": [
    "# データセットを確認する\n",
    "train_dataset = image_loader.LoadDataSet(\n",
    "    setting.const.TRAIN_PATH,\n",
    "    const.IMG_HEIGHT,\n",
    "    const.IMG_WIDTH,\n",
    "    transform=image_loader.get_train_transform(const.IMG_HEIGHT, const.IMG_WIDTH),\n",
    ")\n",
    "\n",
    "# 辞書型のときに要素を取得するマジックメソッド. 以下と同じ意味\n",
    "# image, mask = train_dataset.__getitem__(0)\n",
    "image, mask = train_dataset[0]\n",
    "print(image.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "# Print total number of unique images.\n",
    "# フォルダーの長さを表示. 以下と同じ意味\n",
    "train_dataset.__len__()\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "executionInfo": {
     "elapsed": 2070,
     "status": "ok",
     "timestamp": 1686121124681,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "XZo509n13VnO",
    "outputId": "3cea1123-e7ad-4e74-8f23-7a88b89d4671"
   },
   "outputs": [],
   "source": [
    "def format_image(img):\n",
    "    img = np.array(np.transpose(img, (1, 2, 0)))\n",
    "    # 下は画像拡張での正規化を元に戻しています\n",
    "    mean = np.array((0.485, 0.456, 0.406))\n",
    "    std = np.array((0.229, 0.224, 0.225))\n",
    "    img = std * img + mean\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def format_mask(mask):\n",
    "    mask = np.squeeze(np.transpose(mask, (1, 2, 0)))\n",
    "    # TODO:なぜかLoadDataSetで反転する\n",
    "    mask = np.rot90(mask, k=3)\n",
    "    mask = np.flip(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def visualize_dataset(n_images, num_range, predict=None):\n",
    "    # TODO:表示時に水平・垂直クリップがかるからかからないようにする(Predictedのほうも)\n",
    "    images = random.sample(range(0, num_range), n_images)\n",
    "    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))\n",
    "    print(images)\n",
    "    for i in range(0, len(images)):\n",
    "        img_no = images[i]\n",
    "        image, mask = train_dataset[i]\n",
    "        image = format_image(image)\n",
    "        mask = format_mask(mask)\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[i, 0].set_title(f\"Input Image No.{img_no+1}\")\n",
    "        ax[i, 1].set_title(f\"Label Mask No.{img_no+1}\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num_range = len(train_dataset)\n",
    "visualize_dataset(3, num_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1686121128316,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "kJiD_NPdvl9K",
    "outputId": "7f01c7d9-9c67-4ba9-b497-2bb339f80593"
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.25\n",
    "train_size = int(np.round(train_dataset.__len__() * (1 - split_ratio), 0))\n",
    "valid_size = int(np.round(train_dataset.__len__() * split_ratio, 0))\n",
    "train_data, valid_data = random_split(train_dataset, [train_size, valid_size])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_data, batch_size=10)\n",
    "\n",
    "print(\"Length of train data: {}\".format(len(train_data)))\n",
    "print(\"Length of validation data: {}\".format(len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1686121130753,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "Lv-c6LuDvl9L"
   },
   "outputs": [],
   "source": [
    "# UNet\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        # 資料中の『FCN』に当たる部分\n",
    "        self.conv1 = conv_bn_relu(input_channels, 64)\n",
    "        self.conv2 = conv_bn_relu(64, 128)\n",
    "        self.conv3 = conv_bn_relu(128, 256)\n",
    "        self.conv4 = conv_bn_relu(256, 512)\n",
    "        self.conv5 = conv_bn_relu(512, 1024)\n",
    "        self.down_pooling = nn.MaxPool2d(2)\n",
    "\n",
    "        # 資料中の『Up Sampling』に当たる部分\n",
    "        self.up_pool6 = up_pooling(1024, 512)\n",
    "        self.conv6 = conv_bn_relu(1024, 512)\n",
    "        self.up_pool7 = up_pooling(512, 256)\n",
    "        self.conv7 = conv_bn_relu(512, 256)\n",
    "        self.up_pool8 = up_pooling(256, 128)\n",
    "        self.conv8 = conv_bn_relu(256, 128)\n",
    "        self.up_pool9 = up_pooling(128, 64)\n",
    "        self.conv9 = conv_bn_relu(128, 64)\n",
    "        self.conv10 = nn.Conv2d(64, output_channels, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 正規化\n",
    "        x = x / 255.0\n",
    "\n",
    "        # 資料中の『FCN』に当たる部分\n",
    "        x1 = self.conv1(x)\n",
    "        p1 = self.down_pooling(x1)\n",
    "        x2 = self.conv2(p1)\n",
    "        p2 = self.down_pooling(x2)\n",
    "        x3 = self.conv3(p2)\n",
    "        p3 = self.down_pooling(x3)\n",
    "        x4 = self.conv4(p3)\n",
    "        p4 = self.down_pooling(x4)\n",
    "        x5 = self.conv5(p4)\n",
    "\n",
    "        # 資料中の『Up Sampling』に当たる部分, torch.catによりSkip Connectionをしている\n",
    "        p6 = self.up_pool6(x5)\n",
    "        x6 = torch.cat([p6, x4], dim=1)\n",
    "        x6 = self.conv6(x6)\n",
    "\n",
    "        p7 = self.up_pool7(x6)\n",
    "        x7 = torch.cat([p7, x3], dim=1)\n",
    "        x7 = self.conv7(x7)\n",
    "\n",
    "        p8 = self.up_pool8(x7)\n",
    "        x8 = torch.cat([p8, x2], dim=1)\n",
    "        x8 = self.conv8(x8)\n",
    "\n",
    "        p9 = self.up_pool9(x8)\n",
    "        x9 = torch.cat([p9, x1], dim=1)\n",
    "        x9 = self.conv9(x9)\n",
    "\n",
    "        output = self.conv10(x9)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# 畳み込みとバッチ正規化と活性化関数Reluをまとめている\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def down_pooling():\n",
    "    return nn.MaxPool2d(2)\n",
    "\n",
    "\n",
    "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
    "    return nn.Sequential(\n",
    "        # 転置畳み込み\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size, stride=stride\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1686121134015,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "a7B85skevl9L"
   },
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        # comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.0 * intersection + smooth) / (\n",
    "            inputs.sum() + targets.sum() + smooth\n",
    "        )\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction=\"mean\")\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        # comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = F.sigmoid(inputs)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.0 * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1686121136662,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "R868b0zHvl9L"
   },
   "outputs": [],
   "source": [
    "class IoU(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoU, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection\n",
    "\n",
    "        IoU = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "        return IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147699,
     "status": "ok",
     "timestamp": 1686121370842,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "A69rHo-DnKJ_",
    "outputId": "ba11e3e9-e709-464b-d836-b44941076e4e"
   },
   "outputs": [],
   "source": [
    "# <---------------各インスタンス作成---------------------->\n",
    "model = UNet(3, 1).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = DiceLoss()\n",
    "accuracy_metric = IoU()\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "best_model_file = \"/best_model.pth\"\n",
    "\n",
    "total_train_loss = []\n",
    "total_train_score = []\n",
    "total_valid_loss = []\n",
    "total_valid_score = []\n",
    "\n",
    "losses_value = 0\n",
    "for epoch in range(const.NUM_EPOCHS):\n",
    "    # <---------------トレーニング---------------------->\n",
    "    train_loss = []\n",
    "    train_score = []\n",
    "    valid_loss = []\n",
    "    valid_score = []\n",
    "    pbar = tqdm(train_loader, desc=\"description\")\n",
    "    for x_train, y_train in pbar:\n",
    "        x_train = torch.autograd.Variable(x_train).cuda()\n",
    "        y_train = torch.autograd.Variable(y_train).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        ## 損失計算\n",
    "        loss = criterion(output, y_train)\n",
    "        losses_value = loss.item()\n",
    "        ## 精度評価\n",
    "        score = accuracy_metric(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(losses_value)\n",
    "        train_score.append(score.item())\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, loss: {losses_value}, IoU: {score}\")\n",
    "    # <---------------評価---------------------->\n",
    "    with torch.no_grad():\n",
    "        for image, mask in val_loader:\n",
    "            image = torch.autograd.Variable(image).cuda()\n",
    "            mask = torch.autograd.Variable(mask).cuda()\n",
    "            output = model(image)\n",
    "            ## 損失計算\n",
    "            loss = criterion(output, mask)\n",
    "            losses_value = loss.item()\n",
    "            ## 精度評価\n",
    "            score = accuracy_metric(output, mask)\n",
    "            valid_loss.append(losses_value)\n",
    "            valid_score.append(score.item())\n",
    "\n",
    "    total_train_loss.append(np.mean(train_loss))\n",
    "    total_train_score.append(np.mean(train_score))\n",
    "    total_valid_loss.append(np.mean(valid_loss))\n",
    "    total_valid_score.append(np.mean(valid_score))\n",
    "    print(f\"Train Loss: {total_train_loss[-1]}, Train IOU: {total_train_score[-1]}\")\n",
    "    print(f\"Valid Loss: {total_valid_loss[-1]}, Valid IOU: {total_valid_score[-1]}\")\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"valid_loss_min\": total_valid_loss[-1],\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    ## 容量をとるので必要になったら保存する\n",
    "    # checkpoint_file = \"/checkpoint_{}_weight.pth\".format(epoch+1)\n",
    "    # checkpointの保存\n",
    "    # torch.save(checkpoint, CHECKPOINT_PATH_UNet + checkpoint_file)\n",
    "\n",
    "    # 評価データにおいて最高精度のモデルのcheckpointの保存\n",
    "    if total_valid_loss[-1] <= valid_loss_min:\n",
    "        print(\n",
    "            \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                valid_loss_min, total_valid_loss[-1]\n",
    "            )\n",
    "        )\n",
    "        torch.save(checkpoint, const.CHECKPOINT_PATH_UNet + best_model_file)\n",
    "        valid_loss_min = total_valid_loss[-1]\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 1172,
     "status": "ok",
     "timestamp": 1686121445890,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "D1hbsKbtnKJ_",
    "outputId": "d42f833f-6e50-4397-b618-34c3413f6676"
   },
   "outputs": [],
   "source": [
    "# # 数値の配列を文字列にして返す関数\n",
    "# def convert_list_el_to_str_from_num(arr_num, num_decimals = 0):\n",
    "#     arr_str = []\n",
    "#     for num in arr_num:\n",
    "#         if num_decimals == 0:\n",
    "#             stg = str(num)\n",
    "#         else:\n",
    "#             stg = str(math.floor(num * 10 ** num_decimals) / (10 ** num_decimals))\n",
    "#             while True:\n",
    "#                 if not(stg[-1] in [\"0\", \".\"]):\n",
    "#                     break\n",
    "#                 elif stg == \"0.0\":\n",
    "#                     stg = \"0\"\n",
    "#                 else:\n",
    "#                     stg = stg[:-1]\n",
    "\n",
    "#         arr_str.append(stg)\n",
    "#     return arr_str\n",
    "\n",
    "\n",
    "def convert_list_el_to_str_from_num(arr_num, num_decimals=0):\n",
    "    print(arr_num)\n",
    "    arr_str = []\n",
    "    for num in arr_num:\n",
    "        if num_decimals == 0:\n",
    "            stg = str(num)\n",
    "        else:\n",
    "            num_foor = math.floor(num * 10**num_decimals) / (10**num_decimals)\n",
    "            print(\"num_foor:\" + str(num_foor))\n",
    "            stg = str(num_foor)\n",
    "            print(\"stg:\" + stg)\n",
    "            while True:\n",
    "                if not (stg[-1] in [\"0\", \".\"]):\n",
    "                    break\n",
    "                elif stg == \"0.0\":\n",
    "                    stg = \"0\"\n",
    "                    break\n",
    "                else:\n",
    "                    stg = stg[:-1]\n",
    "                # pdb.set_trace()\n",
    "\n",
    "        print(\"append:\" + stg)\n",
    "        arr_str.append(stg)\n",
    "    return arr_str\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(15, 5))\n",
    "# sns.set_style(style=\"darkgrid\")\n",
    "\n",
    "if const.NUM_EPOCHS < 20:\n",
    "    arange_num = np.arange(1, const.NUM_EPOCHS + 1, step=1)\n",
    "    arange_str = convert_list_el_to_str_from_num(arange_num)\n",
    "\n",
    "    # print(arange_str)\n",
    "    plt.xticks(arange_num, arange_str)\n",
    "    plt.xlim(1, const.NUM_EPOCHS)\n",
    "elif const.NUM_EPOCHS < 60:\n",
    "    plt.xticks(np.arange(1, const.NUM_EPOCHS + 1, step=5))\n",
    "    plt.xlim(0, const.NUM_EPOCHS)\n",
    "else:\n",
    "    plt.xticks(np.arange(1, const.NUM_EPOCHS + 1, step=10))\n",
    "    plt.xlim(0, const.NUM_EPOCHS)\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"xtick.major.width\"] = 1.0\n",
    "plt.rcParams[\"ytick.major.width\"] = 1.0\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.0\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "arange_num = np.arange(0, 0.25 + 0.01, step=0.05)\n",
    "# arange_str = convert_list_el_to_str_from_num(arange_num, 0)\n",
    "# めんどくさいから自分で指定する\n",
    "arange_str = [\"0\", \"0.05\", \"0.1\", \"0.15\", \"0.2\", \"0.25\"]\n",
    "# print(arange_str)\n",
    "plt.yticks(arange_num, arange_str)\n",
    "plt.ylim(0, 0.25)\n",
    "plt.minorticks_on()\n",
    "sns.lineplot(x=range(1, const.NUM_EPOCHS + 1), y=total_train_loss, label=\"Train Loss\")\n",
    "sns.lineplot(x=range(1, const.NUM_EPOCHS + 1), y=total_valid_loss, label=\"Valid Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"DiceLoss\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "arange_num = np.arange(0.65, 1 + 0.01, step=0.05)\n",
    "# arange_str = convert_list_el_to_str_from_num(arange_num, 2)\n",
    "arange_str = [\"0.65\", \"0.7\", \"0.75\", \"0.8\", \"0.85\", \"0.9\", \"0.95\", \"1\"]\n",
    "# print(arange_str)\n",
    "plt.yticks(arange_num, arange_str)\n",
    "plt.ylim(0.65, 1)\n",
    "plt.minorticks_on()\n",
    "sns.lineplot(x=range(1, const.NUM_EPOCHS + 1), y=total_train_score, label=\"Train Score\")\n",
    "sns.lineplot(x=range(1, const.NUM_EPOCHS + 1), y=total_valid_score, label=\"Valid Score\")\n",
    "plt.title(\"Score (IoU)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1686121452695,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "qyoe30NJnKJ_"
   },
   "outputs": [],
   "source": [
    "# bestmodelの読み込み\n",
    "checkpoint = torch.load(const.CHECKPOINT_PATH_UNet + best_model_file)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "start_epoch = checkpoint[\"epoch\"]\n",
    "valid_loss_min = checkpoint[\"valid_loss_min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6100,
     "status": "ok",
     "timestamp": 1686121460748,
     "user": {
      "displayName": "佐藤太洋",
      "userId": "13036286077109574300"
     },
     "user_tz": -540
    },
    "id": "DBoOgWBxnKKA",
    "outputId": "585c5525-f7e2-4bb0-8b2a-274436818098"
   },
   "outputs": [],
   "source": [
    "def visualize_predict(model, n_images, num_range):\n",
    "    images = random.sample(range(0, num_range), n_images)\n",
    "    print(images)\n",
    "    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 18))\n",
    "\n",
    "    # now = datetime.now()\n",
    "    # formatted_time = now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # output_directory = f\"{const.APP_PATH}/tmp/{const.TRAIN_DIR}/{formatted_time}\"\n",
    "    # if not os.path.exists(output_directory):\n",
    "    #     os.makedirs(output_directory)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, mask in val_loader:\n",
    "            data = torch.autograd.Variable(data, volatile=True).cuda()\n",
    "            mask = torch.autograd.Variable(mask, volatile=True).cuda()\n",
    "            o = model(data)\n",
    "            break\n",
    "    for i in range(0, len(images)):\n",
    "        img_no = images[i]\n",
    "        tm = o[i][0].data.cpu().numpy()\n",
    "        img = data[i].data.cpu().numpy()\n",
    "        msk = mask[i].data.cpu().numpy()\n",
    "        img = format_image(img)\n",
    "        msk = format_mask(msk)\n",
    "        ax[i, 0].imshow(img)\n",
    "        ax[i, 1].imshow(msk, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[i, 2].imshow(tm, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[i, 0].set_title(f\"Input Image No.{img_no+1}\")\n",
    "        ax[i, 1].set_title(f\"Label Mask No.{img_no+1}\")\n",
    "        ax[i, 2].set_title(f\"Predicted Mask No.{img_no+1}\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "        ax[i, 2].set_axis_off()\n",
    "        # plt.imsave(f\"{output_directory}/Input_Image_No_{img_no+1}.png\", img)\n",
    "        # plt.imsave(f\"{output_directory}/Label_Mask_No_{img_no+1}.png\", msk)\n",
    "        # plt.imsave(f\"{output_directory}/Predicted_Mask_No_{img_no+1}.png\", tm)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_predict(model, 6, num_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84ezOFPzemHY"
   },
   "outputs": [],
   "source": [
    "def visualize_full_predict(model):\n",
    "    full_dataset = image_loader.LoadDataSet(\n",
    "        setting.const.TRAIN_PATH,\n",
    "        const.IMG_HEIGHT,\n",
    "        const.IMG_WIDTH,\n",
    "        transform=image_loader.get_train_transform(\n",
    "            const.IMG_HEIGHT, const.IMG_WIDTH, horizontal_flip=0.0, vertical_flip=0.0\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    all_loader = DataLoader(dataset=full_dataset, batch_size=full_dataset.__len__())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, mask in all_loader:\n",
    "            data = torch.autograd.Variable(data, volatile=True).cuda()\n",
    "            mask = torch.autograd.Variable(mask, volatile=True).cuda()\n",
    "            o = model(data)\n",
    "            break\n",
    "\n",
    "    n_images = len(data)\n",
    "    print(n_images)\n",
    "    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 180))\n",
    "\n",
    "    now = datetime.now()\n",
    "    formatted_time = now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    output_directory = f\"{const.APP_PATH}/tmp/{const.TRAIN_DIR}/{formatted_time}\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        img_no = i\n",
    "        tm = o[i][0].data.cpu().numpy()\n",
    "        img = data[i].data.cpu().numpy()\n",
    "        msk = mask[i].data.cpu().numpy()\n",
    "        img = format_image(img)\n",
    "        msk = format_mask(msk)\n",
    "        ax[i, 0].imshow(img)\n",
    "        ax[i, 1].imshow(msk, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[i, 2].imshow(tm, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[i, 0].set_title(f\"Input Image No.{img_no+1}\")\n",
    "        ax[i, 1].set_title(f\"Label Mask No.{img_no+1}\")\n",
    "        ax[i, 2].set_title(f\"Predicted Mask No.{img_no+1}\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "        ax[i, 2].set_axis_off()\n",
    "        plt.imsave(f\"{output_directory}/Input_Image_No_{img_no+1}.png\", img)\n",
    "        plt.imsave(f\"{output_directory}/Label_Mask_No_{img_no+1}.png\", msk)\n",
    "        plt.imsave(f\"{output_directory}/Predicted_Mask_No_{img_no+1}.png\", tm)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_full_predict(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
