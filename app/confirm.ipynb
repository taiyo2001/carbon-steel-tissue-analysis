{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30647,"status":"ok","timestamp":1685624720274,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"-QrsJIci8koH","outputId":"9f2bf1db-16a7-40da-cc92-6d15752d7722"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/LAPTOP-3BHDM0TI/windows_research/workspace/research/carbon-steel-tissue-analysis\n","total 41\n","drwx------ 2 root root 4096 May 24 09:49 app\n","drwx------ 2 root root 4096 May 24 09:49 bin\n","drwx------ 2 root root 4096 May 24 09:52 data\n","drwx------ 2 root root 4096 May 27 10:23 .git\n","-rw------- 1 root root 3338 May 27 11:29 .gitignore\n","drwx------ 2 root root 4096 Jun  1 12:40 lib\n","-rw------- 1 root root  265 May  8 09:51 Makefile\n","drwx------ 2 root root 4096 May 27 10:32 memo\n","drwx------ 2 root root 4096 Jun  1 12:48 module\n","-rw------- 1 root root  410 May 27 11:23 README.md\n","drwx------ 2 root root 4096 May 25 11:02 stage1_train\n","drwx------ 2 root root 4096 May 24 00:57 tmp\n"]}],"source":["# ドライブのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 適宜自分のApplicationPATHまで変更\n","APP_PATH = '/content/drive/Othercomputers/LAPTOP-3BHDM0TI/windows_research/workspace/research/carbon-steel-tissue-analysis'\n","\n","import os\n","import time\n","import copy\n","from collections import defaultdict\n","import torch\n","import shutil\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, utils\n","from torch import nn\n","import albumentations as A\n","# from albumentations.pytorch import ToTensor removed\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm as tqdm\n","\n","from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n","import cv2\n","\n","from torch.autograd import Variable\n","from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n","from torch.optim import Adam, SGD\n","import torch.nn.functional as F\n","import zipfile\n","\n","import random\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","TRAIN_PATH = APP_PATH + '/stage1_train'\n","CHECKPOINT_PATH = APP_PATH + '/data/model'\n","\n","os.chdir(APP_PATH)\n","!pwd\n","!ls -al"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nLMch089N2C"},"outputs":[],"source":["#画像データ拡張の関数\n","def get_train_transform():\n","   return A.Compose(\n","       [\n","        #リサイズ(こちらはすでに適用済みなのでなくても良いです)\n","        A.Resize(256, 256),\n","        #正規化(こちらの細かい値はalbumentations.augmentations.transforms.Normalizeのデフォルトの値を適用)\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        #水平フリップ（pはフリップする確率）\n","        A.HorizontalFlip(p=0.25),\n","        #垂直フリップ\n","        A.VerticalFlip(p=0.25),\n","        # ToTensor()\n","        ToTensorV2(),\n","        ])\n","\n","#Datasetクラスの定義\n","class LoadDataSet(Dataset):\n","        def __init__(self,path, transform=None):\n","            self.path = path\n","            self.folders = os.listdir(path)\n","            self.transforms = get_train_transform()\n","\n","        def __len__(self):\n","            return len(self.folders)\n","\n","        def __getitem__(self,idx):\n","            image_folder = os.path.join(self.path,self.folders[idx],'images/')\n","            mask_folder = os.path.join(self.path,self.folders[idx],'masks/')\n","            image_path = os.path.join(image_folder,os.listdir(image_folder)[0])\n","\n","            #画像データの取得\n","            img = io.imread(image_path)[:,:,:3].astype('float32')\n","            img = transform.resize(img,(256,256))\n","\n","            mask = self.get_mask(mask_folder, 256, 256 ).astype('float32')\n","\n","            augmented = self.transforms(image=img, mask=mask)\n","            img = augmented['image']\n","            mask = augmented['mask']\n","            # print(\"mask:{}\".format(mask[0].dim())) # 2次元\n","\n","            # 次元の順番を入れ替えているだけだから手動で修正する\n","            ################################\n","            # x = torch.view_as_real(mask)\n","            ################################\n","            # mask = torch.permute(mask[0], (2, 0, 1))\n","            return (img,mask)\n","\n","        #マスクデータの取得\n","        def get_mask(self,mask_folder,IMG_HEIGHT, IMG_WIDTH):\n","            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n","            for mask_ in os.listdir(mask_folder):\n","                    mask_ = io.imread(os.path.join(mask_folder,mask_))\n","                    mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))\n","                    mask_ = np.expand_dims(mask_,axis=-1)\n","                    mask = np.maximum(mask, mask_)\n","\n","            return mask\n","\n","\n","train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())\n","\n","\n","image, mask = train_dataset.__getitem__(0)\n","print(image.shape)\n","print(mask.shape)\n","\n","#Print total number of unique images.\n","train_dataset.__len__()\n","\n","\n","def format_image(img):\n","    img = np.array(np.transpose(img, (1,2,0)))\n","    #下は画像拡張での正規化を元に戻しています\n","    mean=np.array((0.485, 0.456, 0.406))\n","    std=np.array((0.229, 0.224, 0.225))\n","    img  = std * img + mean\n","    img = img*255\n","    img = img.astype(np.uint8)\n","    return img\n","\n","def format_mask(mask):\n","    mask = np.squeeze(np.transpose(mask, (1,2,0)))\n","    return mask\n","\n","def visualize_dataset(n_images, predict=None):\n","    images = random.sample(range(0, 670), n_images)\n","    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))\n","    print(images)\n","    for i in range(0, len(images)):\n","        img_no = images[i]\n","        image, mask = train_dataset.__getitem__(img_no)\n","        image = format_image(image)\n","        mask = format_mask(mask)\n","        ax[i, 0].imshow(image)\n","        ax[i, 1].imshow(mask, interpolation=\"nearest\", cmap=\"gray\")\n","        ax[i, 0].set_title(\"Input Image\")\n","        ax[i, 1].set_title(\"Label Mask\")\n","        ax[i, 0].set_axis_off()\n","        ax[i, 1].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()\n","\n","visualize_dataset(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJiD_NPdvl9K"},"outputs":[],"source":["split_ratio = 0.25\n","train_size=int(np.round(train_dataset.__len__()*(1 - split_ratio),0))\n","valid_size=int(np.round(train_dataset.__len__()*split_ratio,0))\n","train_data, valid_data = random_split(train_dataset, [train_size, valid_size])\n","train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n","val_loader = DataLoader(dataset=valid_data, batch_size=10)\n","\n","print(\"Length of train　data: {}\".format(len(train_data)))\n","print(\"Length of validation　data: {}\".format(len(valid_data)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv-c6LuDvl9L"},"outputs":[],"source":["# UNet\n","class UNet(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super().__init__()\n","        # 資料中の『FCN』に当たる部分\n","        self.conv1 = conv_bn_relu(input_channels,64)\n","        self.conv2 = conv_bn_relu(64, 128)\n","        self.conv3 = conv_bn_relu(128, 256)\n","        self.conv4 = conv_bn_relu(256, 512)\n","        self.conv5 = conv_bn_relu(512, 1024)\n","        self.down_pooling = nn.MaxPool2d(2)\n","\n","        # 資料中の『Up Sampling』に当たる部分\n","        self.up_pool6 = up_pooling(1024, 512)\n","        self.conv6 = conv_bn_relu(1024, 512)\n","        self.up_pool7 = up_pooling(512, 256)\n","        self.conv7 = conv_bn_relu(512, 256)\n","        self.up_pool8 = up_pooling(256, 128)\n","        self.conv8 = conv_bn_relu(256, 128)\n","        self.up_pool9 = up_pooling(128, 64)\n","        self.conv9 = conv_bn_relu(128, 64)\n","        self.conv10 = nn.Conv2d(64, output_channels, 1)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        # 正規化\n","        x = x/255.\n","\n","        # 資料中の『FCN』に当たる部分\n","        x1 = self.conv1(x)\n","        p1 = self.down_pooling(x1)\n","        x2 = self.conv2(p1)\n","        p2 = self.down_pooling(x2)\n","        x3 = self.conv3(p2)\n","        p3 = self.down_pooling(x3)\n","        x4 = self.conv4(p3)\n","        p4 = self.down_pooling(x4)\n","        x5 = self.conv5(p4)\n","\n","        # 資料中の『Up Sampling』に当たる部分, torch.catによりSkip Connectionをしている\n","        p6 = self.up_pool6(x5)\n","        x6 = torch.cat([p6, x4], dim=1)\n","        x6 = self.conv6(x6)\n","\n","        p7 = self.up_pool7(x6)\n","        x7 = torch.cat([p7, x3], dim=1)\n","        x7 = self.conv7(x7)\n","\n","        p8 = self.up_pool8(x7)\n","        x8 = torch.cat([p8, x2], dim=1)\n","        x8 = self.conv8(x8)\n","\n","        p9 = self.up_pool9(x8)\n","        x9 = torch.cat([p9, x1], dim=1)\n","        x9 = self.conv9(x9)\n","\n","        output = self.conv10(x9)\n","        output = torch.sigmoid(output)\n","\n","        return output\n","\n","#畳み込みとバッチ正規化と活性化関数Reluをまとめている\n","def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","    return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","    )\n","\n","def down_pooling():\n","    return nn.MaxPool2d(2)\n","\n","def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n","    return nn.Sequential(\n","        #転置畳み込み\n","        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7B85skevl9L"},"outputs":[],"source":["class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = F.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE\n","\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        #inputs = F.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","\n","        return 1 - dice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lw8crVe2vl9L"},"outputs":[],"source":["class IoU(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(IoU, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        total = (inputs + targets).sum()\n","        union = total - intersection\n","\n","        IoU = (intersection + smooth)/(union + smooth)\n","\n","        return IoU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R868b0zHvl9L","outputId":"ac4c54cb-9b5d-459c-83f2-51942b8e51d0"},"outputs":[{"ename":"NameError","evalue":"name 'UNet' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#<---------------各インスタンス作成---------------------->\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m UNet(\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(),lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m)\n\u001b[0;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m DiceLoss()\n","\u001b[1;31mNameError\u001b[0m: name 'UNet' is not defined"]}],"source":["#<---------------各インスタンス作成---------------------->\n","model = UNet(3,1).cuda()\n","optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n","criterion = DiceLoss()\n","accuracy_metric = IoU()\n","num_epochs=2\n","valid_loss_min = np.Inf\n","\n","MODEL_FILE = '/model_cell_weight.pth'\n","best_model_file = '/best_model_cell_weight.pth'\n","# checkpoint_path = 'model/chkpoint_'\n","# best_model_path = 'model/bestmodel.pt'\n","\n","total_train_loss = []\n","total_train_score = []\n","total_valid_loss = []\n","total_valid_score = []\n","\n","losses_value = 0\n","for epoch in range(num_epochs):\n","  #<---------------トレーニング---------------------->\n","    train_loss = []\n","    train_score = []\n","    valid_loss = []\n","    valid_score = []\n","    pbar = tqdm(train_loader, desc = 'description')\n","    for x_train, y_train in pbar:\n","      x_train = torch.autograd.Variable(x_train).cuda()\n","      y_train = torch.autograd.Variable(y_train).cuda()\n","      optimizer.zero_grad()\n","      output = model(x_train)\n","      ## 損失計算\n","      loss = criterion(output, y_train)\n","      losses_value = loss.item()\n","      ## 精度評価\n","      score = accuracy_metric(output,y_train)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss.append(losses_value)\n","      train_score.append(score.item())\n","      pbar.set_description(f\"Epoch: {epoch+1}, loss: {losses_value}, IoU: {score}\")\n","    #<---------------評価---------------------->\n","    with torch.no_grad():\n","      for image,mask in val_loader:\n","        image = torch.autograd.Variable(image).cuda()\n","        mask = torch.autograd.Variable(mask).cuda()\n","        output = model(image)\n","        ## 損失計算\n","        loss = criterion(output, mask)\n","        losses_value = loss.item()\n","        ## 精度評価\n","        score = accuracy_metric(output,mask)\n","        valid_loss.append(losses_value)\n","        valid_score.append(score.item())\n","\n","    total_train_loss.append(np.mean(train_loss))\n","    total_train_score.append(np.mean(train_score))\n","    total_valid_loss.append(np.mean(valid_loss))\n","    total_valid_score.append(np.mean(valid_score))\n","    print(f\"Train Loss: {total_train_loss[-1]}, Train IOU: {total_train_score[-1]}\")\n","    print(f\"Valid Loss: {total_valid_loss[-1]}, Valid IOU: {total_valid_score[-1]}\")\n","\n","    checkpoint = {\n","        'epoch': epoch + 1,\n","        'valid_loss_min': total_valid_loss[-1],\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","\n","    checkpoint_file = \"/checkpoint_{}_cell_weight.pth\".format(epoch+1)\n","\n","    # checkpointの保存\n","    # TODO:save_ckpのベストモデルの保存について調査(ただ保存する関数を作っているだけ)\n","    # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","    torch.save(checkpoint, CHECKPOINT_PATH + checkpoint_file)\n","\n","    # 評価データにおいて最高精度のモデルのcheckpointの保存\n","    if total_valid_loss[-1] <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,total_valid_loss[-1]))\n","        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        torch.save(checkpoint, CHECKPOINT_PATH + best_model_file)\n","        valid_loss_min = total_valid_loss[-1]\n","\n","    print(\"\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk4X6bEQemHX"},"outputs":[],"source":["plt.figure(1)\n","plt.figure(figsize=(15,5))\n","sns.set_style(style=\"darkgrid\")\n","plt.subplot(1, 2, 1)\n","sns.lineplot(x=range(1,num_epochs+1), y=total_train_loss, label=\"Train Loss\")\n","sns.lineplot(x=range(1,num_epochs+1), y=total_valid_loss, label=\"Valid Loss\")\n","plt.title(\"Loss\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"DiceLoss\")\n","\n","plt.subplot(1, 2, 2)\n","sns.lineplot(x=range(1,num_epochs+1), y=total_train_score, label=\"Train Score\")\n","sns.lineplot(x=range(1,num_epochs+1), y=total_valid_score, label=\"Valid Score\")\n","plt.title(\"Score (IoU)\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"IoU\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiN--Ax7emHX"},"outputs":[],"source":["# 学習が完了してからload_ckpに対応したものに変更してbest_model_pathを表示\n","# model = MyModelDefinition(args)\n","# model.load_state_dict(torch.load('load/from/path/model.pth'))\n","\n","checkpoint = torch.load(CHECKPOINT_PATH + best_model_file)\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","start_epoch = checkpoint['epoch']\n","valid_loss_min = checkpoint['valid_loss_min']\n","\n","# https://wandb.ai/wandb_fc/japanese/reports/PyTorch---VmlldzoxNTAyODQy\n","\n","# model, optimizer, start_epoch, valid_loss_min = load_ckp(best_model_path, model, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"va1ja350emHY"},"outputs":[],"source":["def visualize_predict(model, n_images):\n","    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 18))\n","    with torch.no_grad():\n","        for data,mask in val_loader:\n","            data = torch.autograd.Variable(data, volatile=True).cuda()\n","            mask = torch.autograd.Variable(mask, volatile=True).cuda()\n","            o = model(data)\n","            break\n","    for img_no in range(0, n_images):\n","        tm=o[img_no][0].data.cpu().numpy()\n","        img = data[img_no].data.cpu()\n","        msk = mask[img_no].data.cpu()\n","        img = format_image(img)\n","        msk = format_mask(msk)\n","        ax[img_no, 0].imshow(img)\n","        ax[img_no, 1].imshow(msk, interpolation=\"nearest\", cmap=\"gray\")\n","        ax[img_no, 2].imshow(tm, interpolation=\"nearest\", cmap=\"gray\")\n","        ax[img_no, 0].set_title(\"Input Image\")\n","        ax[img_no, 1].set_title(\"Label Mask\")\n","        ax[img_no, 2].set_title(\"Predicted Mask\")\n","        ax[img_no, 0].set_axis_off()\n","        ax[img_no, 1].set_axis_off()\n","        ax[img_no, 2].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()\n","\n","visualize_predict(model, 6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84ezOFPzemHY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VSJUGTwD0LD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rSMP8HZD0LD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y68yN-3HD0LE"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}